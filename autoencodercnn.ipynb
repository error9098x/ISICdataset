{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing import image\nfrom tqdm import tqdm","metadata":{"id":"eeFqVia2hjY_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n\n# # Load the data\n# data = pd.read_csv('/kaggle/working/GT.csv')\n\n# # Remove all rows after the 3000th row\n# data = data.iloc[:2000]\n\n# # Save the updated data to a new CSV file\n# data.to_csv('/kaggle/working/GT.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/isicgt25k/ISIC_2019_Training_GroundTruth.csv')\ndata.head()","metadata":{"id":"pcgFmonEhjgu","outputId":"ac607275-98b6-4214-e969-c68d69aa3d88","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store_list = []\nimage_height = 350\nimage_width = 350\nfor i in tqdm(range(data.shape[0])):\n    path = '/kaggle/input/isic2019/ISIC_2019_Training_Input/' + data['image'][i] + '.jpg'\n    image_check = image.load_img(path, target_size=(image_height, image_width))\n    image_check = image.img_to_array(image_check)\n    # scaling the images\n    image_check = image_check/255\n    store_list.append(image_check)","metadata":{"id":"GNyiyYrmhjo7","outputId":"9fb199b7-f7a8-48dc-e983-765fe18f9eec","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.array(store_list)\ny = data.drop(columns=['image'])\nY = y.to_numpy()","metadata":{"id":"gTQ0wBeUhjsn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, Y, random_state=42, test_size=0.20)\n","metadata":{"id":"yUlf-9D0hjvK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the autoencoder architecture\ninput_img = Input(shape=x_train[0].shape)\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPool2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = MaxPool2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPool2D((2, 2), padding='same')(x)\n\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(16, (3, 3), activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = Model(input_img, decoded)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Train the autoencoder\nautoencoder.fit(x_train, x_train, epochs=10, batch_size=128, validation_data=(x_test, x_test))\n\n# Use the encoder part of the autoencoder to extract features\nencoder = Model(input_img, encoded)\nfeatures_train = encoder.predict(x_train)\nfeatures_test = encoder.predict(x_test)\n\n# Define the classification model architecture\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=features_train[0].shape))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(9, activation='sigmoid'))\nmodel.summary()","metadata":{"id":"8Lzlhasyhjyo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the classification model\nmodel.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\nhistory = model.fit(features_train, y_train, epochs=10, batch_size=128, validation_data=(features_test, y_test))","metadata":{"id":"ryCT6-NEhj1i","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = model.evaluate(features_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\n# Make predictions on the test set\ny_pred = model.predict(features_test)\n\n# Print the actual and predicted data side by side\nfor i in range(len(y_test)):\n    print('Actual:', y_test[i], 'Predicted:', y_pred[i])\n\n# Plot the accuracy and loss graphs\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend()\nplt.show()","metadata":{"id":"TNfbS6erhj36","trusted":true},"execution_count":null,"outputs":[]}]}